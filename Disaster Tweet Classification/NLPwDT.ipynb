{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview)","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U git+https://github.com/huggingface/transformers.git\n!pip install -U git+https://github.com/huggingface/accelerate.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.optim as optim\n\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom transformers import TextClassificationPipeline\n\nfrom datasets import Dataset\n\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nnltk.download('stopwords')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-06-09T15:13:06.982815Z","iopub.execute_input":"2023-06-09T15:13:06.983228Z","iopub.status.idle":"2023-06-09T15:13:16.439379Z","shell.execute_reply.started":"2023-06-09T15:13:06.983190Z","shell.execute_reply":"2023-06-09T15:13:16.438237Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"stopWords = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:16.441547Z","iopub.execute_input":"2023-06-09T15:13:16.442562Z","iopub.status.idle":"2023-06-09T15:13:16.449763Z","shell.execute_reply.started":"2023-06-09T15:13:16.442502Z","shell.execute_reply":"2023-06-09T15:13:16.448643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Dataset and Pre-Processing","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:37.489691Z","iopub.execute_input":"2023-06-09T15:13:37.490114Z","iopub.status.idle":"2023-06-09T15:13:37.547996Z","shell.execute_reply.started":"2023-06-09T15:13:37.490085Z","shell.execute_reply":"2023-06-09T15:13:37.546706Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:38.368446Z","iopub.execute_input":"2023-06-09T15:13:38.368884Z","iopub.status.idle":"2023-06-09T15:13:38.385708Z","shell.execute_reply.started":"2023-06-09T15:13:38.368852Z","shell.execute_reply":"2023-06-09T15:13:38.384490Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_df.drop(columns=[\"location\"], inplace=True)\ntest_df.drop(columns=[\"location\"], inplace=True)\ntrain_df = train_df[train_df[\"keyword\"].isnull() == False]","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:39.126427Z","iopub.execute_input":"2023-06-09T15:13:39.128969Z","iopub.status.idle":"2023-06-09T15:13:39.142051Z","shell.execute_reply.started":"2023-06-09T15:13:39.128909Z","shell.execute_reply":"2023-06-09T15:13:39.140648Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def cleanAndPreProcess(sentence):\n    sentence = str(sentence).lower()\n    sentence = ' '.join(re.findall(\"[A-Za-z]{1,}\", str(re.sub(r\"http\\S+\", \"\", str(sentence)))))\n    words = list(word_tokenize(sentence))\n    list_words = [w for w in words if w not in stopWords]\n    return ' '.join(list_words)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:39.973488Z","iopub.execute_input":"2023-06-09T15:13:39.973925Z","iopub.status.idle":"2023-06-09T15:13:39.980916Z","shell.execute_reply.started":"2023-06-09T15:13:39.973891Z","shell.execute_reply":"2023-06-09T15:13:39.979811Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df[\"clean\"] = train_df[\"text\"].apply(cleanAndPreProcess)\ntest_df[\"clean\"] = test_df[\"text\"].apply(cleanAndPreProcess)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:40.598850Z","iopub.execute_input":"2023-06-09T15:13:40.599266Z","iopub.status.idle":"2023-06-09T15:13:43.126834Z","shell.execute_reply.started":"2023-06-09T15:13:40.599234Z","shell.execute_reply":"2023-06-09T15:13:43.125645Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch Custom Model","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 10000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_df[\"clean\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:43.129579Z","iopub.execute_input":"2023-06-09T15:13:43.129923Z","iopub.status.idle":"2023-06-09T15:13:43.315143Z","shell.execute_reply.started":"2023-06-09T15:13:43.129893Z","shell.execute_reply":"2023-06-09T15:13:43.313946Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_sequences = tokenizer.texts_to_sequences(train_df[\"clean\"])\ntrain_pad = pad_sequences(train_sequences, maxlen=40, padding=\"post\", truncating=\"post\")","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:43.316385Z","iopub.execute_input":"2023-06-09T15:13:43.316746Z","iopub.status.idle":"2023-06-09T15:13:43.486948Z","shell.execute_reply.started":"2023-06-09T15:13:43.316716Z","shell.execute_reply":"2023-06-09T15:13:43.485943Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test_df[\"clean\"])\ntest_pad = pad_sequences(test_sequences, maxlen=40, padding=\"post\", truncating=\"post\")","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:45.081847Z","iopub.execute_input":"2023-06-09T15:13:45.083030Z","iopub.status.idle":"2023-06-09T15:13:45.162131Z","shell.execute_reply.started":"2023-06-09T15:13:45.082981Z","shell.execute_reply":"2023-06-09T15:13:45.161142Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train = torch.Tensor(train_pad).type(torch.int)\ny_train = torch.Tensor(train_df[\"target\"].values)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:13:50.930768Z","iopub.execute_input":"2023-06-09T15:13:50.931145Z","iopub.status.idle":"2023-06-09T15:13:50.975986Z","shell.execute_reply.started":"2023-06-09T15:13:50.931116Z","shell.execute_reply":"2023-06-09T15:13:50.975002Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class DisasterClassifier(nn.Module):\n    \n    def __init__(self):\n        super(DisasterClassifier, self).__init__()\n        \n        self.embedding_layer = nn.Embedding(10000, 64)\n        self.lstm = nn.LSTM(64, 64, bidirectional=True, batch_first=True)\n        self.conv1d = nn.Conv1d(40, 64, 10)\n        self.relu = nn.PReLU()\n        self.flatten = nn.Flatten()\n        self.dense1 = nn.Linear(7616, 512)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense2 = nn.Linear(512, 32)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense3 = nn.Linear(32, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedding_out = self.embedding_layer(x)\n        lstm_out,_ = self.lstm(embedding_out)\n        conv_out = self.relu(self.conv1d(lstm_out))\n        flat_out = self.flatten(conv_out)\n        dense_out1 = self.dropout1(self.dense1(flat_out))\n        dense_out2 = self.dropout2(self.dense2(dense_out1))\n        out = self.sigmoid(self.dense3(dense_out2))\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:14:05.808334Z","iopub.execute_input":"2023-06-09T15:14:05.808778Z","iopub.status.idle":"2023-06-09T15:14:05.820325Z","shell.execute_reply.started":"2023-06-09T15:14:05.808743Z","shell.execute_reply":"2023-06-09T15:14:05.819039Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = DisasterClassifier()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters())\nn_epochs = 25","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:14:07.042348Z","iopub.execute_input":"2023-06-09T15:14:07.043509Z","iopub.status.idle":"2023-06-09T15:14:07.124899Z","shell.execute_reply.started":"2023-06-09T15:14:07.043473Z","shell.execute_reply":"2023-06-09T15:14:07.123700Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    training_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels.reshape(-1,1))\n        loss.backward()\n        optimizer.step()\n        training_loss += loss.item()\n    \n    epoch_loss = training_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {epoch_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:14:07.983976Z","iopub.execute_input":"2023-06-09T15:14:07.984394Z","iopub.status.idle":"2023-06-09T15:20:04.602854Z","shell.execute_reply.started":"2023-06-09T15:14:07.984360Z","shell.execute_reply":"2023-06-09T15:20:04.601648Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/25 - Loss: 0.6348030301979033\nEpoch 2/25 - Loss: 0.4681513832282212\nEpoch 3/25 - Loss: 0.3428021266551341\nEpoch 4/25 - Loss: 0.23256399417769605\nEpoch 5/25 - Loss: 0.16168837237834804\nEpoch 6/25 - Loss: 0.12710489483818538\nEpoch 7/25 - Loss: 0.09955678210530619\nEpoch 8/25 - Loss: 0.07472047223453016\nEpoch 9/25 - Loss: 0.06384552311363346\nEpoch 10/25 - Loss: 0.05188424452587812\nEpoch 11/25 - Loss: 0.05127861446866734\nEpoch 12/25 - Loss: 0.053476772213628974\nEpoch 13/25 - Loss: 0.04622272006692306\nEpoch 14/25 - Loss: 0.04172813624177637\nEpoch 15/25 - Loss: 0.0381583273432995\nEpoch 16/25 - Loss: 0.03471686386827491\nEpoch 17/25 - Loss: 0.03391814491571834\nEpoch 18/25 - Loss: 0.058143377265477574\nEpoch 19/25 - Loss: 0.09956703473783247\nEpoch 20/25 - Loss: 0.10753664115413736\nEpoch 21/25 - Loss: 0.057035554869244\nEpoch 22/25 - Loss: 0.03783325441068204\nEpoch 23/25 - Loss: 0.03198438651671202\nEpoch 24/25 - Loss: 0.03143758807362832\nEpoch 25/25 - Loss: 0.030601340922519246\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    y_pred = model(X_train)\n    y_pred = torch.round(y_pred)\n    accuracy = (y_pred[:,0] == y_train).sum().item()\n    accuracy = accuracy / len(y_train)\n    print(f\"Train Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:04.605668Z","iopub.execute_input":"2023-06-09T15:20:04.606126Z","iopub.status.idle":"2023-06-09T15:20:07.288252Z","shell.execute_reply.started":"2023-06-09T15:20:04.606083Z","shell.execute_reply":"2023-06-09T15:20:07.285996Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Train Accuracy: 0.9850370762711864\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    X_test = torch.Tensor(test_pad).type(torch.int)\n    y_test_pred = model(X_test)\n    y_test_pred = torch.round(y_test_pred).type(torch.int)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:07.289477Z","iopub.execute_input":"2023-06-09T15:20:07.290492Z","iopub.status.idle":"2023-06-09T15:20:08.397936Z","shell.execute_reply.started":"2023-06-09T15:20:07.290457Z","shell.execute_reply":"2023-06-09T15:20:08.396873Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test_df['id'], 'target': y_test_pred[:, 0]})\nsubmission_df.to_csv('/kaggle/working/submission1_NLPwDT.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## HuggingFace Transformers","metadata":{}},{"cell_type":"code","source":"df = train_df[[\"clean\", \"target\"]]\ndf.rename(columns={\"clean\": \"text\", \"target\": \"label\"}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:08.401350Z","iopub.execute_input":"2023-06-09T15:20:08.401845Z","iopub.status.idle":"2023-06-09T15:20:08.411606Z","shell.execute_reply.started":"2023-06-09T15:20:08.401800Z","shell.execute_reply":"2023-06-09T15:20:08.410419Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_339/2799901050.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.rename(columns={\"clean\": \"text\", \"target\": \"label\"}, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:08.413112Z","iopub.execute_input":"2023-06-09T15:20:08.413576Z","iopub.status.idle":"2023-06-09T15:20:12.762910Z","shell.execute_reply.started":"2023-06-09T15:20:08.413519Z","shell.execute_reply":"2023-06-09T15:20:12.761696Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015ea4105dc34b8abcc8088ef6651497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b112936900f54f91a4a4c647a33374b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cf5e3005ade4e3e8f439ba0e76f26f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"decc902cefae4d8aa15084ac8cb7ccb9"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:12.764281Z","iopub.execute_input":"2023-06-09T15:20:12.764708Z","iopub.status.idle":"2023-06-09T15:20:12.770277Z","shell.execute_reply.started":"2023-06-09T15:20:12.764675Z","shell.execute_reply":"2023-06-09T15:20:12.769257Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(df, test_size=0.2)\n\ntrain_ds = Dataset.from_pandas(df_train, split=\"train\")\ntest_ds = Dataset.from_pandas(df_val, split=\"test\")\n\ntokenized_train = train_ds.map(preprocess_function, batched=True)\ntokenized_test = test_ds.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:12.771375Z","iopub.execute_input":"2023-06-09T15:20:12.771807Z","iopub.status.idle":"2023-06-09T15:20:13.290009Z","shell.execute_reply.started":"2023-06-09T15:20:12.771775Z","shell.execute_reply":"2023-06-09T15:20:13.289080Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fd497a772e24c14a27a1053610c8c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a838ddf86694622bf04021a76d2cb3b"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:13.291117Z","iopub.execute_input":"2023-06-09T15:20:13.292088Z","iopub.status.idle":"2023-06-09T15:20:13.298343Z","shell.execute_reply.started":"2023-06-09T15:20:13.292056Z","shell.execute_reply":"2023-06-09T15:20:13.297199Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:13.299947Z","iopub.execute_input":"2023-06-09T15:20:13.300313Z","iopub.status.idle":"2023-06-09T15:20:18.149069Z","shell.execute_reply.started":"2023-06-09T15:20:13.300282Z","shell.execute_reply":"2023-06-09T15:20:18.148164Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e46ac56c6b4fb29578dc9fbb099ddc"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    report_to=\"none\",\n    evaluation_strategy ='steps',\n    metric_for_best_model='f1',\n    logging_steps=100,\n    load_best_model_at_end=True,\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:20:18.152795Z","iopub.execute_input":"2023-06-09T15:20:18.153628Z","iopub.status.idle":"2023-06-09T16:56:12.956919Z","shell.execute_reply.started":"2023-06-09T15:20:18.153582Z","shell.execute_reply":"2023-06-09T16:56:12.954190Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3780' max='3780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3780/3780 1:35:52, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.556000</td>\n      <td>0.459838</td>\n      <td>0.799471</td>\n      <td>0.749794</td>\n      <td>0.806394</td>\n      <td>0.700617</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.450400</td>\n      <td>0.447577</td>\n      <td>0.802780</td>\n      <td>0.763492</td>\n      <td>0.785948</td>\n      <td>0.742284</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.429600</td>\n      <td>0.421490</td>\n      <td>0.819987</td>\n      <td>0.776683</td>\n      <td>0.829825</td>\n      <td>0.729938</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.423600</td>\n      <td>0.428688</td>\n      <td>0.814030</td>\n      <td>0.772470</td>\n      <td>0.812606</td>\n      <td>0.736111</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.372800</td>\n      <td>0.429908</td>\n      <td>0.821310</td>\n      <td>0.782258</td>\n      <td>0.819257</td>\n      <td>0.748457</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.380500</td>\n      <td>0.447335</td>\n      <td>0.812707</td>\n      <td>0.782809</td>\n      <td>0.778626</td>\n      <td>0.787037</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.381000</td>\n      <td>0.443359</td>\n      <td>0.825281</td>\n      <td>0.763016</td>\n      <td>0.912017</td>\n      <td>0.655864</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.335100</td>\n      <td>0.432634</td>\n      <td>0.829914</td>\n      <td>0.785297</td>\n      <td>0.856102</td>\n      <td>0.725309</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.320100</td>\n      <td>0.443711</td>\n      <td>0.830576</td>\n      <td>0.794543</td>\n      <td>0.827759</td>\n      <td>0.763889</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.323800</td>\n      <td>0.465473</td>\n      <td>0.823958</td>\n      <td>0.791536</td>\n      <td>0.804140</td>\n      <td>0.779321</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.325300</td>\n      <td>0.437445</td>\n      <td>0.834547</td>\n      <td>0.798387</td>\n      <td>0.836149</td>\n      <td>0.763889</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.288900</td>\n      <td>0.467754</td>\n      <td>0.827929</td>\n      <td>0.789644</td>\n      <td>0.829932</td>\n      <td>0.753086</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.257900</td>\n      <td>0.511686</td>\n      <td>0.823958</td>\n      <td>0.790551</td>\n      <td>0.807074</td>\n      <td>0.774691</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.269000</td>\n      <td>0.487467</td>\n      <td>0.832561</td>\n      <td>0.794809</td>\n      <td>0.837607</td>\n      <td>0.756173</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.262900</td>\n      <td>0.464210</td>\n      <td>0.836532</td>\n      <td>0.796373</td>\n      <td>0.854867</td>\n      <td>0.745370</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.255900</td>\n      <td>0.525633</td>\n      <td>0.828590</td>\n      <td>0.791633</td>\n      <td>0.826891</td>\n      <td>0.759259</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.219900</td>\n      <td>0.542362</td>\n      <td>0.831899</td>\n      <td>0.797771</td>\n      <td>0.824013</td>\n      <td>0.773148</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.228300</td>\n      <td>0.550904</td>\n      <td>0.832561</td>\n      <td>0.793469</td>\n      <td>0.842288</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.227300</td>\n      <td>0.570579</td>\n      <td>0.819987</td>\n      <td>0.788820</td>\n      <td>0.793750</td>\n      <td>0.783951</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.208600</td>\n      <td>0.579209</td>\n      <td>0.822634</td>\n      <td>0.790297</td>\n      <td>0.801587</td>\n      <td>0.779321</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.192000</td>\n      <td>0.546990</td>\n      <td>0.829252</td>\n      <td>0.790924</td>\n      <td>0.832765</td>\n      <td>0.753086</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.165800</td>\n      <td>0.601801</td>\n      <td>0.828590</td>\n      <td>0.790622</td>\n      <td>0.830221</td>\n      <td>0.754630</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.185900</td>\n      <td>0.605633</td>\n      <td>0.827929</td>\n      <td>0.788961</td>\n      <td>0.832192</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.156100</td>\n      <td>0.644636</td>\n      <td>0.829252</td>\n      <td>0.797170</td>\n      <td>0.812500</td>\n      <td>0.782407</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.197000</td>\n      <td>0.669230</td>\n      <td>0.824619</td>\n      <td>0.792807</td>\n      <td>0.803487</td>\n      <td>0.782407</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.151400</td>\n      <td>0.686737</td>\n      <td>0.824619</td>\n      <td>0.794414</td>\n      <td>0.798752</td>\n      <td>0.790123</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.147700</td>\n      <td>0.682694</td>\n      <td>0.826605</td>\n      <td>0.791733</td>\n      <td>0.816393</td>\n      <td>0.768519</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.187000</td>\n      <td>0.663739</td>\n      <td>0.828590</td>\n      <td>0.789602</td>\n      <td>0.833619</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.149200</td>\n      <td>0.688388</td>\n      <td>0.827267</td>\n      <td>0.790361</td>\n      <td>0.824121</td>\n      <td>0.759259</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.134300</td>\n      <td>0.712413</td>\n      <td>0.823958</td>\n      <td>0.790551</td>\n      <td>0.807074</td>\n      <td>0.774691</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.144900</td>\n      <td>0.730622</td>\n      <td>0.826605</td>\n      <td>0.793701</td>\n      <td>0.810289</td>\n      <td>0.777778</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.136600</td>\n      <td>0.714590</td>\n      <td>0.824619</td>\n      <td>0.788169</td>\n      <td>0.817579</td>\n      <td>0.760802</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.125700</td>\n      <td>0.718227</td>\n      <td>0.821310</td>\n      <td>0.780130</td>\n      <td>0.825862</td>\n      <td>0.739198</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.123900</td>\n      <td>0.749262</td>\n      <td>0.822634</td>\n      <td>0.788976</td>\n      <td>0.805466</td>\n      <td>0.773148</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.131900</td>\n      <td>0.752135</td>\n      <td>0.820649</td>\n      <td>0.786782</td>\n      <td>0.802568</td>\n      <td>0.771605</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.116600</td>\n      <td>0.759291</td>\n      <td>0.820649</td>\n      <td>0.787451</td>\n      <td>0.800638</td>\n      <td>0.774691</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.110700</td>\n      <td>0.752514</td>\n      <td>0.824619</td>\n      <td>0.790182</td>\n      <td>0.811382</td>\n      <td>0.770062</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3780, training_loss=0.24280354232384413, metrics={'train_runtime': 5754.3708, 'train_samples_per_second': 10.498, 'train_steps_per_second': 0.657, 'total_flos': 378581217617940.0, 'train_loss': 0.24280354232384413, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)\nscores = classifier(test_df[\"clean\"].to_list())\nscores_ = [int(label['label'].split(\"_\")[1]) for label in scores]","metadata":{"execution":{"iopub.status.busy":"2023-06-09T16:56:12.961802Z","iopub.execute_input":"2023-06-09T16:56:12.963596Z","iopub.status.idle":"2023-06-09T16:57:55.788462Z","shell.execute_reply.started":"2023-06-09T16:56:12.963528Z","shell.execute_reply":"2023-06-09T16:57:55.787473Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test_df['id'], 'target': scores_})\nsubmission_df.to_csv('/kaggle/working/submission1_NLPwDT.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}